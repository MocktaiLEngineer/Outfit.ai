{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0x2Nj6nYVYx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torchvision\n",
        "import os \n",
        "import pandas as pd\n",
        "from skimage import io\n",
        "from torch.utils.data import (\n",
        "    Dataset,\n",
        "    DataLoader,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kG6GPihWYWMD"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhdGP0GHYZiC"
      },
      "outputs": [],
      "source": [
        "# get the images\n",
        "import zipfile\n",
        "root_path = './'\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/data.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(root_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEEo8QxPMgAJ"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([ transforms.ToPILImage(),\n",
        "                                  transforms.Resize(256),\n",
        "                                  transforms.CenterCrop(224),\n",
        "                                  transforms.ToTensor(),\n",
        "                                  transforms.Normalize([0.485, 0.456, 0.406], \n",
        "                                                       [0.229, 0.224, 0.225])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpZ7JgX-A6at"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2KUsWJ0Mlhm"
      },
      "outputs": [],
      "source": [
        "class FashionDataset(Dataset):\n",
        "  def __init__(self,csv_file,root_dir,transform):\n",
        "    self.annotations = pd.read_csv(csv_file, header = 0)\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "    \n",
        "    self.graphic_id_to_name = dict()\n",
        "    self.graphic_name_to_id = dict()\n",
        "\n",
        "    self.productGroup_id_to_name = dict()\n",
        "    self.productGroup_name_to_id = dict()\n",
        "    \n",
        "    self.productType_id_to_name = dict()\n",
        "    self.productType_name_to_id = dict()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    image_path = os.path.join(self.root_dir, '0' + str(self.annotations.iloc[index,0]) + '.jpg')\n",
        "    image = io.imread(image_path)\n",
        "\n",
        "    self.annotations['graphical_appearance_name'] = self.annotations['graphical_appearance_name'].astype('category')\n",
        "    self.annotations['product_group_name'] = self.annotations['product_group_name'].astype('category')\n",
        "    self.annotations['product_type_name'] = self.annotations['product_type_name'].astype('category')\n",
        "\n",
        "    self.graphic_id_to_name = dict( enumerate(self.annotations['graphical_appearance_name'].cat.categories ) )\n",
        "    self.graphic_name_to_id = dict((v,k) for k,v in self.graphic_id_to_name.items())\n",
        "\n",
        "    self.productGroup_id_to_name = dict( enumerate(self.annotations['product_group_name'].cat.categories ) )\n",
        "    self.productGroup_name_to_id = dict((v,k) for k,v in self.productGroup_id_to_name.items())\n",
        "\n",
        "    self.productType_id_to_name = dict( enumerate(self.annotations['product_type_name'].cat.categories ) )\n",
        "    self.productType_name_to_id = dict((v,k) for k,v in self.productType_id_to_name.items())\n",
        "\n",
        "    # To do changes\n",
        "    self.productGroup_label = torch.tensor(self.productGroup_name_to_id[self.annotations.iloc[index,1]])\n",
        "    self.graphic_label = torch.tensor(self.graphic_name_to_id[self.annotations.iloc[index,2]])\n",
        "    self.productType_label = torch.tensor(self.productType_name_to_id[self.annotations.iloc[index,3]])\n",
        "\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "\n",
        "    # return the image and all the associated labels\n",
        "    dict_data = {\n",
        "        'img': image,\n",
        "        'labels': {\n",
        "            'label_productGroup': self.productGroup_label,\n",
        "            'label_graphic': self.graphic_label,\n",
        "            'label_productType': self.productType_label\n",
        "        }\n",
        "    }\n",
        "    return dict_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wN6ms1FSsPwH"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "learning_rate = 1e-3\n",
        "batch_size = 32\n",
        "num_epochs = 10 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "081LnZt2aPpf"
      },
      "outputs": [],
      "source": [
        "#Load dataset\n",
        "dataset = FashionDataset(csv_file=\"articles_with_attributes_new.csv\",root_dir=\"data\", transform= transform)\n",
        "\n",
        "train_size = int(0.1 * len(dataset))\n",
        "test_size = int(0.1 * len(dataset))\n",
        "validation_size = len(dataset) - (train_size + test_size)\n",
        "\n",
        "train_set,test_set, validation_set = torch.utils.data.random_split(dataset,[train_size,test_size,validation_size])\n",
        "\n",
        "train_loader = DataLoader(train_set,batch_size,shuffle=True,drop_last=True)\n",
        "test_loader = DataLoader(test_set,batch_size,shuffle=True,drop_last=True)\n",
        "validate_loader = DataLoader(validation_set,batch_size,shuffle=True,drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader"
      ],
      "metadata": {
        "id": "M551s7PLM_fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_to_idx = []\n",
        "class_to_idx.append(dataset.productGroup_name_to_id)\n",
        "class_to_idx.append(dataset.graphic_name_to_id)\n",
        "class_to_idx.append(dataset.productType_name_to_id)"
      ],
      "metadata": {
        "id": "HVW_Ia6VFuf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_to_idx"
      ],
      "metadata": {
        "id": "RCkZFqKvGKSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntPOTNyx7DGo"
      },
      "outputs": [],
      "source": [
        "class MultiOutputModel(nn.Module):\n",
        "    def __init__(self, n_product_group_classes, n_graphic_classes, n_product_type_classes):\n",
        "        super().__init__()\n",
        "        self.resnet = models.resnet34(pretrained=True)\n",
        "        self.model_wo_fc = nn.Sequential(*(list(self.resnet.children())[:-1]))\n",
        "\n",
        "        # create separate classifiers for our outputs\n",
        "        self.productGroup = nn.Sequential(\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(in_features=512, out_features=n_product_group_classes)\n",
        "        )\n",
        "        self.graphic = nn.Sequential(\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(in_features=512, out_features=n_graphic_classes)\n",
        "        )\n",
        "        self.productType = nn.Sequential(\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(in_features=512, out_features=n_product_type_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model_wo_fc(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        return {\n",
        "            'productGroup': self.productGroup(x),\n",
        "            'graphic': self.graphic(x),\n",
        "            'productType': self.productType(x)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEiL4rGs9Ua6"
      },
      "outputs": [],
      "source": [
        "col_names = [\"article_id\", \"product_group_name\", \"graphical_appearance_name\", \"product_type_name\"]\n",
        "df = pd.read_csv('articles_with_attributes_new.csv',names = col_names, header= 0 )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "z9ZpgIqi0XdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['product_group_name'].unique()"
      ],
      "metadata": {
        "id": "0XfdY-ePGIEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JnYxgv5AfIY"
      },
      "outputs": [],
      "source": [
        "model = MultiOutputModel(n_product_group_classes=len(df['product_group_name'].unique()),\n",
        "                             n_graphic_classes=len(df['graphical_appearance_name'].unique()),\n",
        "                             n_product_type_classes=len(df['product_type_name'].unique())).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "AEWOhaRqEXJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def criterion(loss_func,outputs,pictures):\n",
        "  losses = 0\n",
        "  for i, key in enumerate(outputs):\n",
        "    losses += loss_func(outputs[key], pictures['labels'][f'label_{key}'].to(device))\n",
        "  return losses\n",
        "\n",
        "def training(model,device,lr_rate,epochs,train_loader):\n",
        "  num_epochs = epochs\n",
        "  losses = []\n",
        "  checkpoint_losses = []\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr_rate)\n",
        "  n_total_steps = len(train_loader)\n",
        "\n",
        "  loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "     for i, pictures in enumerate(train_loader):\n",
        "        images = pictures['img'].to(device)\n",
        "        pictures = pictures\n",
        "\n",
        "        outputs = model(images)\n",
        "\n",
        "        loss = criterion(loss_func,outputs, pictures)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % (int(n_total_steps/1)) == 0:\n",
        "            checkpoint_loss = torch.tensor(losses).mean().item()\n",
        "            checkpoint_losses.append(checkpoint_loss)\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {checkpoint_loss:.4f}')\n",
        "  return checkpoint_losses\n",
        "\n",
        "checkpoint_losses = training(model,device,0.0001,10,train_loader)"
      ],
      "metadata": {
        "id": "2h1j-YPPInd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the model\n",
        "torch.save(model.state_dict(), \"PyTorch-MultiLabelClassification-resnet34.pth\")"
      ],
      "metadata": {
        "id": "e6I7qnIxN-4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiOutputModel(n_product_group_classes=len(df['product_group_name'].unique()),\n",
        "                             n_graphic_classes=len(df['graphical_appearance_name'].unique()),\n",
        "                             n_product_type_classes=len(df['product_type_name'].unique())).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"PyTorch-MultiLabelClassification-resnet34.pth\"))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "X2XuZI6TOMdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class color:\n",
        "   RED = '\\033[91m'\n",
        "   BOLD = '\\033[1m'\n",
        "   END = '\\033[0m'"
      ],
      "metadata": {
        "id": "nwf_QkSfO86d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(model, dataloader, *args):\n",
        "\n",
        "  all_predictions = torch.tensor([]).to(device)\n",
        "  all_true_labels = torch.tensor([]).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    n_correct = []\n",
        "    n_class_correct = []\n",
        "    n_class_samples = []\n",
        "    n_samples = 0\n",
        "\n",
        "    for arg in args:\n",
        "      n_correct.append(len(arg))\n",
        "      n_class_correct.append([0 for i in range(len(arg))])\n",
        "      n_class_samples.append([0 for i in range(len(arg))])\n",
        "\n",
        "    print(\"Length Of Dataloader\",len(dataloader))\n",
        "    \n",
        "    for pictures in dataloader:\n",
        "      images = pictures['img'].to(device)\n",
        "\n",
        "      outputs = model(images)\n",
        "      labels = [pictures['labels'][picture].to(device) for picture in pictures['labels']]\n",
        "\n",
        "      for i,out in enumerate(outputs):\n",
        "\n",
        "        _, predicted = torch.max(outputs[out],1)\n",
        "        n_correct[i] += (predicted == labels[i]).sum().item()\n",
        "\n",
        "        if i == 0:\n",
        "          n_samples += labels[i].size(0)\n",
        "\n",
        "        for k in range(32):\n",
        "          # print(\"label\",labels[i][k])\n",
        "          # print(\"label[i]\",labels[i])\n",
        "          label = labels[i][k]\n",
        "          pred = predicted[k]\n",
        "          if (label == pred):\n",
        "              n_class_correct[i][label] += 1\n",
        "          n_class_samples[i][label] += 1\n",
        "          \n",
        "  return n_correct,n_samples,n_class_correct,n_class_samples\n",
        "\n",
        "def class_acc(n_correct,n_samples,n_class_correct,n_class_samples,class_list):\n",
        "    for i in range(len(class_list)):\n",
        "      print(\"-------------------------------------------------\")\n",
        "      acc = 100.0 * n_correct[i] / n_samples\n",
        "      print(color.BOLD + color.RED + f'Overall class performance: {round(acc,1)} %' + color.END)\n",
        "      for k in range(len(class_list[i])):\n",
        "          if int(n_class_samples[i][k]) == 0:\n",
        "            acc = 100.0\n",
        "          else:\n",
        "            acc = 100.0 * n_class_correct[i][k] / n_class_samples[i][k]\n",
        "            print(f'Accuracy of {class_list[i][k]}: {round(acc,1)} %')\n",
        "    print(\"-------------------------------------------------\")\n",
        "\n",
        "\n",
        "classes_productGroup = list(dataset.productGroup_id_to_name.values())\n",
        "classes_graphic = list(dataset.graphic_id_to_name.values())\n",
        "classes_productType = list(dataset.productType_id_to_name.values())\n",
        "\n",
        "class_list = [classes_productGroup,classes_graphic,classes_productType]\n",
        "\n",
        "n_correct,n_samples,n_class_correct,n_class_samples = validation(model,test_loader,classes_productGroup,classes_graphic,classes_productType)\n",
        "\n",
        "class_acc(n_correct,n_samples,n_class_correct,n_class_samples,class_list)"
      ],
      "metadata": {
        "id": "e1uwGVCsLzzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "-UIghGM0BMlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model._modules['model_wo_fc']._modules['8']"
      ],
      "metadata": {
        "id": "Fk1z0vTS_h4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Use the model object to select the desired layer\n",
        "layer = model._modules['model_wo_fc']._modules['8']\n",
        "print(layer)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "                                  transforms.Resize(256),\n",
        "                                  transforms.CenterCrop(224),\n",
        "                                  transforms.ToTensor(),\n",
        "                                  transforms.Normalize([0.485, 0.456, 0.406], \n",
        "                                                       [0.229, 0.224, 0.225])])\n",
        "\n",
        "def get_vector(image):\n",
        "    # Create a PyTorch tensor with the transformed image\n",
        "    t_img = transform(image)\n",
        "    # Create a vector of zeros that will hold our feature vector\n",
        "    # The 'avgpool' layer has an output size of 512\n",
        "    my_embedding = torch.zeros(512)\n",
        "\n",
        "    # Define a function that will copy the output of a layer\n",
        "    def copy_data(m, i, o):\n",
        "        my_embedding.copy_(o.flatten())                 # <-- flatten\n",
        "\n",
        "    # Attach that function to our selected layer\n",
        "    h = layer.register_forward_hook(copy_data)\n",
        "    # Run the model on our transformed image\n",
        "    with torch.no_grad():                               # <-- no_grad context\n",
        "        model(t_img.unsqueeze(0))                       # <-- unsqueeze\n",
        "    # Detach our copy function from the layer\n",
        "    h.remove()\n",
        "    # Return the feature vector\n",
        "    return my_embedding\n"
      ],
      "metadata": {
        "id": "IhMC8TAO-vBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getEmbeddings(img_path):\n",
        "\n",
        "  img = Image.open(img_path)\n",
        "  embeddings = get_vector(img)\n",
        "\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "QTHvVUD9BXvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.cpu()"
      ],
      "metadata": {
        "id": "9B49YhGGaHCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from multiprocessing import  Pool\n",
        "import numpy as np\n",
        "\n",
        "def parallelize_dataframe(df, func, n_cores=4):\n",
        "    df_split = np.array_split(df, n_cores)\n",
        "    pool = Pool(n_cores)\n",
        "    df = pd.concat(pool.map(func, df_split))\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    return df\n",
        "\n",
        "def getEmbeddings(img_path):\n",
        "\n",
        "  image_path = os.path.join('data',img_path)\n",
        "  img = Image.open(image_path)\n",
        "  embeddings = get_vector(img)\n",
        "\n",
        "  return embeddings\n",
        "\n",
        "def getEmbeddingsOnDataset(df):\n",
        "  df['Embeddings'] = df['image_path'].apply(lambda x: getEmbeddings(x))\n",
        "  return df\n",
        "\n",
        "train_df = pd.read_csv('articles_with_attributes_new.csv')\n",
        "train_df['image_path'] = train_df['article_id'].apply(lambda x: '0' + str(x) + '.jpg')\n",
        "train = parallelize_dataframe(train_df, getEmbeddingsOnDataset)"
      ],
      "metadata": {
        "id": "kjaXUySkaCZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train.to_pickle('multiattribute_with_embeddings.pkl')    #to save the dataframe, df to 123.pkl\n",
        "# train = pd.read_pickle('multiattribute_with_embeddings.pkl') #to load 123.pkl back to the dataframe df"
      ],
      "metadata": {
        "id": "bH-x_1hhGdZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.to_csv('attribute_articles_with_embeddings.csv',index=False)\n",
        "\n",
        "!cp -r 'attribute_articles_with_embeddings.csv' '/content/drive/MyDrive/'"
      ],
      "metadata": {
        "id": "RBVXaTSVzKlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r '/content/drive/MyDrive/attribute_articles_with_embeddings.csv' './'"
      ],
      "metadata": {
        "id": "l_HE4nXcq_pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(r'attribute_articles_with_embeddings.csv')"
      ],
      "metadata": {
        "id": "ctMKqKnorR-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install annoy"
      ],
      "metadata": {
        "id": "qh-D3SGAAXEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from annoy import AnnoyIndex\n",
        "f = len(train['Embeddings'][0])\n",
        "t = AnnoyIndex(f, metric='euclidean')\n",
        "\n",
        "ntree = 1000 # hyper-parameter, the more the number of trees better the prediction\n",
        "for i, vector in enumerate(train['Embeddings']):\n",
        "    t.add_item(i, vector)\n",
        "_  = t.build(ntree)"
      ],
      "metadata": {
        "id": "GbiWRW1yAaSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def get_similar_images_annoy(img_index):\n",
        "    start = time.time()\n",
        "    base_img_id, base_vector, productGroup_label, graphic_label, productType_label  = train.iloc[img_index, [0,5,1,2,3]]\n",
        "    similar_img_ids = t.get_nns_by_item(img_index, 8)\n",
        "    end = time.time()\n",
        "    print(f'{(end - start) * 1000} ms')\n",
        "    return base_img_id, productGroup_label, graphic_label, productType_label, train.iloc[similar_img_ids]"
      ],
      "metadata": {
        "id": "x6YD01ZPBryG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_img_id, productGroup_label, graphic_label, productType_label, similar_images_df = get_similar_images_annoy(29187)"
      ],
      "metadata": {
        "id": "nPXCISPRE-gE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[train['article_id'] == 693678001]"
      ],
      "metadata": {
        "id": "KA1C7EzPF5ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similar_images_df"
      ],
      "metadata": {
        "id": "XHul43ZaFJfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r '/content/drive/MyDrive/multiattribute_with_embeddings.pkl' './'"
      ],
      "metadata": {
        "id": "-82V5jNwuDS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_pickle('multiattribute_with_embeddings.pkl')"
      ],
      "metadata": {
        "id": "zGrY0CHIuJzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def get_similar_images_annoy(img_index):\n",
        "    start = time.time()\n",
        "    base_img_id, base_vector, productGroup_label, graphic_label, productType_label  = train.iloc[img_index, [0,5,1,2,3]]\n",
        "    similar_img_ids = t.get_nns_by_item(img_index, 8)\n",
        "    end = time.time()\n",
        "    print(f'{(end - start) * 1000} ms')\n",
        "    return base_img_id, productGroup_label, graphic_label, productType_label, train.iloc[similar_img_ids]"
      ],
      "metadata": {
        "id": "6to0HSzZuQ9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def process_image(image_path):\n",
        "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
        "        returns an Numpy array\n",
        "    '''\n",
        "    \n",
        "    # Process a PIL image for use in a PyTorch model\n",
        "    \n",
        "    pil_image = Image.open(image_path)\n",
        "    \n",
        "    # Resize\n",
        "    if pil_image.size[0] > pil_image.size[1]:\n",
        "        pil_image.thumbnail((5000, 256))\n",
        "    else:\n",
        "        pil_image.thumbnail((256, 5000))\n",
        "        \n",
        "    # Crop \n",
        "    left_margin = (pil_image.width-224)/2\n",
        "    bottom_margin = (pil_image.height-224)/2\n",
        "    right_margin = left_margin + 224\n",
        "    top_margin = bottom_margin + 224\n",
        "    \n",
        "    pil_image = pil_image.crop((left_margin, bottom_margin, right_margin, top_margin))\n",
        "    \n",
        "    # Normalize\n",
        "    np_image = np.array(pil_image)/255\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    np_image = (np_image - mean) / std\n",
        "\n",
        "    np_image = np_image.transpose((2, 0, 1))\n",
        "    \n",
        "    return np_image"
      ],
      "metadata": {
        "id": "G0QF0dMuB7x9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement the code to predict the class from an image file\n",
        "\n",
        "def predict(image_path, model, topk=1):\n",
        "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
        "    '''\n",
        "    \n",
        "    image = process_image(image_path)\n",
        "    \n",
        "    # Convert image to PyTorch tensor first\n",
        "    image = torch.from_numpy(image).type(torch.cuda.FloatTensor)\n",
        "    #print(image.shape)\n",
        "    #print(type(image))\n",
        "    \n",
        "    # Returns a new tensor with a dimension of size one inserted at the specified position.\n",
        "    image = image.unsqueeze(0)\n",
        "    \n",
        "    output = model.forward(image)\n",
        "    \n",
        "    attributes = []\n",
        "\n",
        "    for i,label in enumerate(output):\n",
        "      output_tensor = output.get(label)\n",
        "\n",
        "      print(\"output_tensor\", output_tensor)\n",
        "      \n",
        "      _,pred = torch.max(output_tensor, dim = 1)\n",
        "\n",
        "      print(\"pred\", pred)\n",
        "    \n",
        "      # # Probabilities and the indices of those probabilities corresponding to the classes\n",
        "      # top_probabilities, top_indices = probabilities.topk(topk)\n",
        "    \n",
        "      # # Convert to lists\n",
        "      # top_probabilities = top_probabilities.detach().type(torch.FloatTensor).numpy().tolist()[0] \n",
        "      # top_indices = top_indices.detach().type(torch.FloatTensor).numpy().tolist()[0] \n",
        "    \n",
        "      # Convert topk_indices to the actual class labels using class_to_idx\n",
        "      # Invert the dictionary so you get a mapping from index to class.\n",
        "      \n",
        "      idx_to_class = {value: key for key, value in class_to_idx[i].items()}\n",
        "      \n",
        "      # top_classes = [idx_to_class[index] for index in top_indices]\n",
        "\n",
        "      class_label = idx_to_class[pred.detach().type(torch.FloatTensor).numpy()[0]]\n",
        "      print(\"class_label\", class_label)\n",
        "      \n",
        "      attributes.append(class_label)\n",
        "    return attributes\n",
        "    \n",
        "classes = predict('0305304008.jpg', model)   \n",
        "print(classes)"
      ],
      "metadata": {
        "id": "0xLXjCRiY9sh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "PyTorch-MultiLabelClassification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}